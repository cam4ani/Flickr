{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import numpy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "#to set connection with Flickr API\n",
    "from flickrapi import FlickrAPI\n",
    "\n",
    "#image\n",
    "from PIL import Image\n",
    "\n",
    "#url open to get image\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "\n",
    "#date\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "#plot (for image verification)\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFO on flickr\n",
    "#for more parameter options: https://www.flickr.com/services/api/flickr.photos.search.html\n",
    "#tags (Optional): A comma-delimited list of tags. Photos with one or more (or all tags by changing tags_mode)of the \n",
    "#tags listed will be returned. You can exclude results that match a term by prepending it with a - character.\n",
    "#http://joequery.me/code/flickr-api-image-search-python/\n",
    "#lisence info: https://www.flickr.com/services/api/flickr.photos.licenses.getInfo.html\n",
    "#geolocalisation should not be used, as for example picture might be taken from a museum. we should add geolocalisation \n",
    "#based on 'biology' knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### type of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = 'oiseau_du_rhone'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of species and synonymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataframe must at least have 'Species' columns for the name of each species, and a 'li_synonyms_final' columns with list of\n",
    "#their synonyms without including itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>scientific_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cygne tuberculé</td>\n",
       "      <td>Cygnus olor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cygne de Bewick</td>\n",
       "      <td>Cygnus columbianus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cygne chanteur</td>\n",
       "      <td>Cygnus cygnus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Species     scientific_name\n",
       "0  Cygne tuberculé         Cygnus olor\n",
       "1  Cygne de Bewick  Cygnus columbianus\n",
       "2   Cygne chanteur       Cygnus cygnus"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_species = pd.read_csv(os.path.join(ROOT_DIR, 'oiseau_du_rhone','ListeOiseauxRhone27-05-2016_simpler.csv'), sep=';')\n",
    "print(df_species.shape)\n",
    "df_species.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>li_synonyms_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cygne tuberculé</td>\n",
       "      <td>Cygnus olor</td>\n",
       "      <td>[Cygnus olor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cygne de Bewick</td>\n",
       "      <td>Cygnus columbianus</td>\n",
       "      <td>[Cygnus columbianus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cygne chanteur</td>\n",
       "      <td>Cygnus cygnus</td>\n",
       "      <td>[Cygnus cygnus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Species     scientific_name     li_synonyms_final\n",
       "0  Cygne tuberculé         Cygnus olor         [Cygnus olor]\n",
       "1  Cygne de Bewick  Cygnus columbianus  [Cygnus columbianus]\n",
       "2   Cygne chanteur       Cygnus cygnus       [Cygnus cygnus]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a columns with their synonyms\n",
    "df_species['li_synonyms_final'] = df_species['scientific_name'].map(lambda x: [x] if str(x)!='nan' else [])\n",
    "#dont forget to do map(lambda x: eval(x)) if not saw as a list\n",
    "df_species.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the dataframe is in adequate format\n",
    "if not 'Species' in df_species.columns:\n",
    "    print('ERROR: you must have Species as a colomn name with the representative name for each species')\n",
    "if not 'li_synonyms_final' in df_species.columns:\n",
    "    print('ERROR: you must have li_synonyms_final as a colomn name with a list fo synonym for each species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get these data fllow the direction of: http://joequery.me/code/flickr-api-image-search-python/\n",
    "FLICKR_PUBLIC = 'dd0cb0ced4e83452f8d49cb3d534707d'\n",
    "FLICKR_SECRET = '4c568d2002b5506e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr = FlickrAPI(FLICKR_PUBLIC, FLICKR_SECRET, format='parsed-json')\n",
    "extras = 'description,geo,tags,url_c,owner_name,date_taken,license'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create fodlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case to erase all\n",
    "#shutil.rmtree(os.path.join(path_data,type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(ROOT_DIR,'datasets',type_)\n",
    "#create a director if not existing for images\n",
    "if not os.path.exists(path_data):\n",
    "    os.makedirs(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 350 species\n"
     ]
    }
   ],
   "source": [
    "#create a list of species\n",
    "li_species = df_species['Species'].tolist()\n",
    "li_species = [x for x in li_species if str(x) != 'nan']\n",
    "#verify unicity of species name\n",
    "if len(li_species)!=len(set(li_species)):\n",
    "    #search for the duplicate species\n",
    "    df_ = df_species['Species'].value_counts().reset_index()\n",
    "    li_duplicate = df_[df_['Species']!=1]['Species'].tolist()\n",
    "    print('ERROR: non unique species name (%d species in total)'%len(li_duplicate))\n",
    "    print('the following first 10 species appear more than ones: %s'%' \\-\\ '.join(li_duplicate[0:10]))\n",
    "    sys.exit()\n",
    "print('There is %d species'%len(li_species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a director if not existing for images\n",
    "if type_ not in [x.split('\\\\')[-1] for x in glob.glob(os.path.join(ROOT_DIR,'*'))]:\n",
    "    print('ERROR: your type images doe snot exist as a fodler in the Flickr folder')\n",
    "    sys.exit()\n",
    "p = os.path.join(ROOT_DIR,'datasets',type_)\n",
    "if not os.path.exists(p):\n",
    "    os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one folder per species folder if not existing\n",
    "for species in li_species:\n",
    "    folder_path_s = os.path.join(path_data,species)\n",
    "    if not os.path.exists(folder_path_s):\n",
    "        os.makedirs(folder_path_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check amount of flickr images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 495 images collected from Flickr\n"
     ]
    }
   ],
   "source": [
    "#keeping meta data of only the images we truely have \n",
    "#look at the actual image we really have\n",
    "li_flickr_images = []\n",
    "for species in glob.glob(os.path.join(path_data,'*')):\n",
    "    li_flickr_images.extend([x for x in glob.glob(os.path.join(species,'*')) if x.endswith('.png')])\n",
    "len(li_flickr_images)\n",
    "print('We have %d images collected from Flickr'%len(li_flickr_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download image from flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea: collect all the image from the begining date, and until no more new image are outcome. In this way one can \n",
    "#rerun at anytime to grab only the new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percent of the species were already requested until date 17_01_2019\n"
     ]
    }
   ],
   "source": [
    "#choose starting date and we will take species that was not taken at this starting date\n",
    "date = '17_01_2019' #in string otherwise might change if we run over two days\n",
    "f = os.path.join(path_data,'li_species_done_'+date+'.pkl')\n",
    "if len(glob.glob(f))>0:\n",
    "    li_species_done = pickle.load(open(f, 'rb'))\n",
    "else:\n",
    "    li_species_done = []\n",
    "print('%d percent of the species were already requested until date %s'%(len(li_species_done)/len(li_species)*100,\n",
    "                                                                        date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to overcome: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)>, form urlopen(url)\n",
    "gcontext = ssl.SSLContext(ssl.PROTOCOL_TLSv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 350 species left to query for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cygnus olor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Initializing from file failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c422f3c3891a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_meta_data_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;31m#date will be: Timestamp('2017-04-30 17:12:52')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mdf_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_meta_data_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetaken'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[1;31m#we need to take minus one day as min_taken_date is apparently working at day level, and before saving\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m#we'll need to remove possibly duplicates (might happen if several picture taken the same day but we stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Initializing from file failed"
     ]
    }
   ],
   "source": [
    "#download images form flickr\n",
    "#Go in each species folder, and downlaod all the photos with a taken date greater than or equal to the maximum one \n",
    "#recorded in the species-metadata file if it exist, otherwise download it from the begining (\"0000-00-00 00:00:00\")\n",
    "#While downloading an image, if there is an error from flickr stop the code (might be connection error). Then you \n",
    "#simply need to rerun it perhaps few minutes later\n",
    "li_species_to_do = [x for x in li_species if x not in li_species_done]\n",
    "print('We have %d species left to query for'%len(li_species_to_do))\n",
    "\n",
    "for nbr, species in tqdm.tqdm(enumerate(li_species_to_do)):\n",
    "    \n",
    "    #save all previous species as done until that specific date\n",
    "    li_species_done = li_species[0:nbr]\n",
    "    pickle.dump(li_species_done, open(os.path.join(path_data,'li_species_done_'+date+'.pkl'), 'wb'))\n",
    "    \n",
    "    #list of synonyms for the species\n",
    "    li_syn = df_species[df_species['Species']==species]['li_synonyms_final'].values[0] + [species]\n",
    "    \n",
    "    #initialize folder path for this species\n",
    "    folder_path_s = os.path.join(path_data, species)\n",
    "    \n",
    "    #iterate through each species synonyms\n",
    "    for species_word in li_syn:\n",
    "        print(species_word)\n",
    "        \n",
    "        #initialization\n",
    "        t = \"0000-00-00 00:00:00\"\n",
    "        df_old = pd.DataFrame()\n",
    "    \n",
    "        #if the new collection of images is empty, then stop it, otherwise continue with the last taken date\n",
    "        while True:\n",
    "\n",
    "            #open the existing metadata file if any, and use the max taken date to grab data from that point instead\n",
    "            old_meta_data_file = os.path.join(folder_path_s,'flickr_df_'+species+'.csv')\n",
    "            if len(glob.glob(old_meta_data_file))>0:\n",
    "                #date will be: Timestamp('2017-04-30 17:12:52')\n",
    "                df_old = pd.read_csv(old_meta_data_file, parse_dates=['datetaken'], index_col=False, sep=';', \n",
    "                                     engine='python') #engine='python': to avoid OSError: Initializing from file failed\n",
    "                #we need to take minus one day as min_taken_date is apparently working at day level, and before saving\n",
    "                #we'll need to remove possibly duplicates (might happen if several picture taken the same day but we stop\n",
    "                #at a \"middle picture of the day\"). Also we convert to good format for flickr query\n",
    "                df_ = df_old[df_old['species_word']==species_word].copy()\n",
    "                if df_.shape[0]>0:\n",
    "                    t = (max(df_['datetaken'].tolist()) - dt.timedelta(days=1)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    #print('we will use as starting date %s'%str(t))\n",
    "            \n",
    "            #take at most 10 times if their is a connection error connection error )\n",
    "            k = 0\n",
    "            while k<10:\n",
    "                try:\n",
    "                    image_data = flickr.photos.search(text='\\\"'+species+'\\\"', content_type=1, media=\"photos\", \n",
    "                                                      per_page=500, extras=extras, min_taken_date=t)\n",
    "                    k = 10\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except ConnectionError as e:\n",
    "                    k = k+1\n",
    "                    print('DOWNLOAD ISSUE for species %s, due to error: %s, lets SLEEP'%(species,e))\n",
    "                    # sleep for 5 seconds\n",
    "                    time.sleep(5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print('DOWNLOAD ISSUE for species %s, due to error: %s, lets STOP'%(species, e))\n",
    "                    image_data = None\n",
    "                    k = 10\n",
    "                    \n",
    "            if image_data==None:\n",
    "                print('image is none get out of loop')\n",
    "                break\n",
    "                \n",
    "            #download image with its url and save it if its a new one\n",
    "            for i, photo in enumerate(image_data['photos']['photo']): #besides photos there is only a 'stat' key\n",
    "                if 'url_c' in photo:\n",
    "                    url = photo['url_c']\n",
    "                    try:\n",
    "                        if len(glob.glob(os.path.join(folder_path_s,species+'_'+photo['id']+\".png\")))==0:\n",
    "                            img = Image.open(urlopen(url, context=gcontext))\n",
    "                            img.save(os.path.join(folder_path_s,species+'_'+photo['id']+\".png\"))\n",
    "                            del img\n",
    "                    except KeyboardInterrupt:\n",
    "                        raise\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print('SAVE ISSUE for species %s and url %s'%(species,str(url)))\n",
    "\n",
    "            #create new metadata file with all the images (old and new)\n",
    "            df_new = pd.DataFrame(image_data['photos']['photo'])\n",
    "            df_new['species_word'] = species_word\n",
    "            df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "\n",
    "            #save and remove duplicates (first, uniform the id type(as when we save and open the str get converted \n",
    "            #to int))\n",
    "            if df.shape[0]>0:\n",
    "                df['id'] = df['id'].map(lambda x: int(x))\n",
    "                #drop duplicates due to dates that must overlap when re-query data for the second time\n",
    "                #we keep trace of each image evn if its already find for another syn, in this way we would directly know\n",
    "                #which image respond ti which species-word, and also which last-taken date correspond to which species\n",
    "                df = df.drop_duplicates(subset=['id', 'species_word'], keep='first', inplace=False)\n",
    "                #save metadata for each images of this species (note: might be empty if no images was collected)\n",
    "                df.to_csv(os.path.join(folder_path_s,'flickr_df_'+species+'.csv'), index=False, sep=';')\n",
    "\n",
    "            #print(df_old.shape,df_new.shape,df.shape) #to debug\n",
    "            #if there was already data collected and the new one brought some more data (not tru now that we have several\n",
    "            #names per species)\n",
    "            #if (df_old.shape[0]>0) and (df.shape[0]>df_old.shape[0]):\n",
    "            #    print('species %s needed two collected data'%species)\n",
    "            #if no more data was bring last time\n",
    "            if df.shape[0]==df_old.shape[0]: #wrong: df_new.shape[0]==0: indeed we can gather images that are already (-1d)\n",
    "                del image_data\n",
    "                break\n",
    "\n",
    "#do_request: Status code 502 received\n",
    "#('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
    "#tatus code 500 received : problem is with the website itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create one csv file with all metadata info from each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "li_df = []\n",
    "for species in tqdm.tqdm(glob.glob(os.path.join(path_ml,'datasets','flickr_images','*'))):\n",
    "    csv_f = glob.glob(os.path.join(species,'*.csv'))\n",
    "    if len(csv_f)==1:\n",
    "        df = pd.read_csv(csv_f[0], sep=';', index_col=False)\n",
    "        df['species'] = species.split('/')[-1]\n",
    "        li_df.append(df)\n",
    "        del df\n",
    "df_all = pd.concat(li_df, ignore_index=True)\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['img_path'] = df_all.apply(lambda x: os.path.join(folder_path,x['species'], \n",
    "                                                         x['species']+'_'+str(x['id'])+\".png\"), axis=1)\n",
    "print(df_all.shape)\n",
    "#keeping only the images we truely have \n",
    "df_all = df_all[df_all['img_path'].isin(li_flickr_images)]\n",
    "print(df_all.shape)\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save metadata info (might not be of same size of number of collected images)\n",
    "df_all.to_csv(os.path.join(path_data,'flickr_image_info.csv'),index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
